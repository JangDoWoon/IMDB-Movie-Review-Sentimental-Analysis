{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ts9-VQyi9I_x"
   },
   "source": [
    "1. You can pick one of the models. Please feel free to use pytorch/tensorflow tutorials or any \n",
    "existing source codes. There is a restriction to choosing a topic. The model and source code \n",
    "you pick should be different from your final project topic. It could be related but the source \n",
    "code should be different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlJPpAdT9N1Q"
   },
   "source": [
    "IMDB Movie Review Sentimental Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWPBlKkrHlv-"
   },
   "source": [
    "IMDB Large Movie Dataset consists of 50,000 movie review texts written in English, with a label of 1 positive and 0 negative. Of these, 25000 are contained to be used as training data, and the remaining 25000 are contained to be used as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czO8_pvCHhRe"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0BPmHMMIqtt"
   },
   "outputs": [],
   "source": [
    "# load IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCR-Zk3KI1us",
    "outputId": "dc7e208d-136a-482a-d2af-ba9b941d7801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "1\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "# check the data\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "print(len(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NilxyyCpJGss"
   },
   "outputs": [],
   "source": [
    "# laod word_index\n",
    "word_to_index = keras.datasets.imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-YiWxZ1pLHcj",
    "outputId": "eb621cc6-d50f-439b-8ee4-d5ef1dd3e9c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hq0U2lp-KyNB"
   },
   "outputs": [],
   "source": [
    "# padding \n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=500)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5cUymm5Lkoq",
    "outputId": "e61fd1e8-96a1-4a05-9a4a-8c3dcfa4fdef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 36s 108ms/step - loss: 0.4475 - accuracy: 0.7901 - val_loss: 0.3722 - val_accuracy: 0.8406\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.3072 - accuracy: 0.8730 - val_loss: 0.3648 - val_accuracy: 0.8500\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2495 - accuracy: 0.9036 - val_loss: 0.3638 - val_accuracy: 0.8576\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2001 - accuracy: 0.9256 - val_loss: 0.3343 - val_accuracy: 0.8588\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1581 - accuracy: 0.9417 - val_loss: 0.3879 - val_accuracy: 0.8642\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1977 - accuracy: 0.9211 - val_loss: 0.4134 - val_accuracy: 0.8410\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1637 - accuracy: 0.9388 - val_loss: 0.4203 - val_accuracy: 0.8632\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1097 - accuracy: 0.9620 - val_loss: 0.5008 - val_accuracy: 0.8424\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1645 - accuracy: 0.9348 - val_loss: 0.4788 - val_accuracy: 0.8650\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.0811 - accuracy: 0.9717 - val_loss: 0.5899 - val_accuracy: 0.8450\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "vocab_size = 5000    # Size of vocaburary\n",
    "word_vector_dim = 300  # Word vector dimension\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(120))   \n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) # positive or negative\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omPs9SvPVbL5"
   },
   "source": [
    "It seems difficult for word vectors to learn significantly just by briefly learning the emotional classification task. It is said that it is difficult to learn word vectors with this level of training data. Therefore, let's use a pre-learned word embedding model called Word2Vec provided by Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9jaqqyKTCYa"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQxI_lVdWLKu"
   },
   "outputs": [],
   "source": [
    "# setting embedding layer\n",
    "vocab_size = 5000   \n",
    "word_vector_dim = 300 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32tNe1hFZ0Qz"
   },
   "outputs": [],
   "source": [
    "# copy Word2Vec vector in the embedding_matrix\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsxznEMsZ9mR",
    "outputId": "58407a3c-a98a-4801-ebfb-9758e15472a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 36s 108ms/step - loss: 0.5397 - accuracy: 0.7153 - val_loss: 0.3922 - val_accuracy: 0.8354\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.3275 - accuracy: 0.8633 - val_loss: 0.3429 - val_accuracy: 0.8532\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.2546 - accuracy: 0.9009 - val_loss: 0.3219 - val_accuracy: 0.8632\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1987 - accuracy: 0.9255 - val_loss: 0.3365 - val_accuracy: 0.8706\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1559 - accuracy: 0.9439 - val_loss: 0.3676 - val_accuracy: 0.8580\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1341 - accuracy: 0.9514 - val_loss: 0.4210 - val_accuracy: 0.8648\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1146 - accuracy: 0.9586 - val_loss: 0.4280 - val_accuracy: 0.8600\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.0782 - accuracy: 0.9742 - val_loss: 0.5024 - val_accuracy: 0.8580\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.0752 - accuracy: 0.9730 - val_loss: 0.4987 - val_accuracy: 0.8610\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.0524 - accuracy: 0.9830 - val_loss: 0.5466 - val_accuracy: 0.8644\n"
     ]
    }
   ],
   "source": [
    "# trian model\n",
    "\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=500, \n",
    "                                 trainable=True))  # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.LSTM(128))\n",
    "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IGhmv7gZ_QS",
    "outputId": "b260c4cb-5f14-41b6-acb0-503948f7a879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 [==============================] - 33s 99ms/step - loss: 0.5399 - accuracy: 0.7115 - val_loss: 0.3764 - val_accuracy: 0.8408\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 30s 96ms/step - loss: 0.3085 - accuracy: 0.8773 - val_loss: 0.3109 - val_accuracy: 0.8716\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 30s 96ms/step - loss: 0.2119 - accuracy: 0.9186 - val_loss: 0.2841 - val_accuracy: 0.8828\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 30s 96ms/step - loss: 0.1494 - accuracy: 0.9457 - val_loss: 0.3038 - val_accuracy: 0.8722\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 30s 96ms/step - loss: 0.1008 - accuracy: 0.9645 - val_loss: 0.4270 - val_accuracy: 0.8704\n"
     ]
    }
   ],
   "source": [
    "# train with GRU\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=500, \n",
    "                                 trainable=True))  # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.GRU(128))\n",
    "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62IXjXxPzRhQ"
   },
   "source": [
    "(a) Code should be run already by you. At the end of the code, please describe what you’ve\n",
    "learned in this practice or any observations (at least three sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGf4566dzW_E"
   },
   "source": [
    "It was my first time doing NLP, and i wanted to use the LSTM i learned in class. First of all, what i learned while implementing the model is how the text enters the deep learnning model. I learned how to handel letters when they are different in length and how to handel the first observed word. Secondly, it was an opportunity to study GRU as well as LSTM learned in class. With this opportunity, i will try more interesting research by applying NLP to my research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TW79neYzT1V"
   },
   "source": [
    "(b) Extra Bonus, Given the code, can you try to improve the performances? Please describe\n",
    "your strategy and show the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEuQxG73Dy_"
   },
   "source": [
    "To impove the model performances, i tried to apply word-to-vec pretrained by big scale dataset. It allows me to pick a vector that can vetter measure the similarity between words. Next, model performance was further improved through GRU instead of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpGEyEF4zWYk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL Hw_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
